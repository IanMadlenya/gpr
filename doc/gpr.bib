@Book{oai:eprints.pascal-network.org:1211,
  title =       "Gaussian Processes for Machine Learning",
  author =      "Carl Edward Rasmussen and Christopher Williams",
  publisher =   "MIT Press",
  year =        "2006",
  abstract =    "Publisher's description: Gaussian processes (GPs)
                 provide a principled, practical, probabilistic approach
                 to learning in kernel machines. GPs have received
                 increased attention in the machine-learning community
                 over the past decade, and this book provides a
                 long-needed systematic and unified treatment of
                 theoretical and practical aspects of GPs in machine
                 learning. The treatment is comprehensive and
                 self-contained, targeted at researchers and students in
                 machine learning and applied statistics. The book deals
                 with the supervised-learning problem for both
                 regression and classification, and includes detailed
                 algorithms. A wide variety of covariance (kernel)
                 functions are presented and their properties discussed.
                 Model selection is discussed both from a Bayesian and a
                 classical perspective. Many connections to other
                 well-known techniques from machine learning and
                 statistics are discussed, including support-vector
                 machines, neural networks, splines, regularization
                 networks, relevance vector machines and others.
                 Theoretical issues including learning curves and the
                 PAC-Bayesian framework are treated, and several
                 approximation methods for learning with large datasets
                 are discussed. The book contains illustrative examples
                 and exercises, and code and datasets are available on
                 the Web. Appendixes provide mathematical background and
                 a discussion of Gaussian Markov processes.",
  bibsource =   "OAI-PMH server at eprints.pascal-network.org",
  oai =         "oai:eprints.pascal-network.org:1211",
  subject =     "Learning/Statistics \& Optimisation; Theory \&
                 Algorithms",
  type =        "NonPeerReviewed",
  URL =         "http://eprints.pascal-network.org/archive/00001211/;
                 http://mitpress.mit.edu/catalog/item/default.asp?ttype=2\&tid=10930",
}

@PhdThesis{SnelsonThesis,
  title =       "Flexible and efficient Gaussian process models for
                 machine learning",
  author =      "Edward Lloyd Snelson",
  year =        "2008",
  month =       feb # "~06",
  abstract =    "2007 I, Edward Snelson, confirm that the work
                 presented in this thesis is my own. Where information
                 has been derived from other sources, I confirm that
                 this has been indi-cated in the thesis. 2 Gaussian
                 process (GP) models are widely used to perform Bayesian
                 nonlinear re-gression and classification --- tasks that
                 are central to many machine learning prob-lems. A GP is
                 nonparametric, meaning that the complexity of the model
                 grows as more data points are received. Another
                 attractive feature is the behaviour of the error bars.
                 They naturally grow in regions away from training data
                 where we have high uncertainty about the interpolating
                 function. In their standard form GPs have several
                 limitations, which can be divided into two broad
                 categories: computational difficulties for large data
                 sets, and restrictive modelling assumptions for complex
                 data sets. This thesis addresses various aspects",
  school = "Gatsby Computational Neuroscience Unit, University College London",
  bibsource =   "OAI-PMH server at citeseerx.ist.psu.edu",
  contributor =  "CiteSeerX",
  language =    "en",
  oai =         "oai:CiteSeerXPSU:10.1.1.62.4041",
  relation =    "10.1.1.28.8311",
  rights =      "Metadata may be used without restrictions as long as
                 the oai identifier remains attached to it.",
  URL =
"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.62.4041;
                 http://www.gatsby.ucl.ac.uk/~snelson/thesis.pdf",
}

@InProceedings{conf/nips/2005,
  title =       "Sparse Gaussian Processes using Pseudo-inputs",
  author =      "Edward Snelson and Zoubin Ghahramani",
  year =        "2005",
  bibdate =     "2006-02-15",
  bibsource =   "DBLP,
                 http://dblp.uni-trier.de/db/conf/nips/nips2005.html#SnelsonG05",
  booktitle =   "NIPS",
  URL =         "http://books.nips.cc/papers/files/nips18/NIPS2005_0543.pdf",
}

@InProceedings{conf/uai/SnelsonG06,
  title =       "Variable Noise and Dimensionality Reduction for Sparse
                 Gaussian processes",
  author =      "Edward Snelson and Zoubin Ghahramani",
  publisher =   "AUAI Press",
  year =        "2006",
  bibdate =     "2007-07-26",
  bibsource =   "DBLP,
                 http://dblp.uni-trier.de/db/conf/uai/uai2006.html#SnelsonG06",
  booktitle =   "UAI",
  ISBN =        "0-9749039-2-2",
  URL =
"http://uai.sis.pitt.edu/displayArticleDetails.jsp?mmnu=1&amp;smnu=2&amp;article_id=1316&amp;proceeding_id=22",
}

@InProceedings{conf/nips/SnelsonRG03,
  title =       "Warped Gaussian Processes",
  author =      "Edward Snelson and Carl Edward Rasmussen and Zoubin
                 Ghahramani",
  publisher =   "MIT Press",
  year =        "2003",
  bibdate =     "2004-10-12",
  bibsource =   "DBLP,
                 http://dblp.uni-trier.de/db/conf/nips/nips2003.html#SnelsonRG03",
  booktitle =   "NIPS",
  crossref =    "conf/nips/2003",
  editor =      "Sebastian Thrun and Lawrence K. Saul and Bernhard
                 Sch{\"o}lkopf",
  ISBN =        "0-262-20152-6",
  URL =         "http://books.nips.cc/papers/files/nips16/NIPS2003_AA43.pdf",
}

@InProceedings{conf/icml/WalderKS08,
  title =       "Sparse multiscale gaussian process regression",
  author =      "Christian Walder and Kwang In Kim and Bernhard
                 Sch{\"o}lkopf",
  bibdate =     "2008-08-14",
  bibsource =   "DBLP,
                 http://dblp.uni-trier.de/db/conf/icml/icml2008.html#WalderKS08",
  booktitle =   "Machine Learning, Proceedings of the Twenty-Fifth
                 International Conference ({ICML} 2008), Helsinki,
                 Finland, June 5-9, 2008",
  publisher =   "ACM",
  year =        "2008",
  volume =      "307",
  editor =      "William W. Cohen and Andrew McCallum and Sam T.
                 Roweis",
  ISBN =        "978-1-60558-205-4",
  pages =       "1112--1119",
  series =      "ACM International Conference Proceeding Series",
  URL =         "http://doi.acm.org/10.1145/1390156.1390296",
}

@Journal{Foster2009,
  author =      "Leslie Foster and Alex Waagen and Nabeela Aijaz and
                 Michael Hurley and Apolonio Luis and Joel Rinsky and
                 Chandrika Satyavolu and Michael J. Way and Paul Gazis
                 and Ashok Srivastava",
  title =       "Stable and Efficient Gaussian Process Calculations",
  journal =     "Journal of Machine Learning Research",
  publisher =   "Microtome Publishing",
  volume =      "10",
  pages =       "857--882",
  ISSN =        "1533-7928 (electronic); 1532-4435 (paper)",
  year =        "2009",
  month =       apr,
  abstract =    "The use of Gaussian processes can be an effective
                 approach to prediction in a supervised learning
                 environment. For large data sets, the standard Gaussian
                 process approach requires solving very large systems of
                 linear equations and approximations are required for
                 the calculations to be practical. We will focus on the
                 subset of regressors approximation technique. We will
                 demonstrate that there can be numerical instabilities
                 in a well known implementation of the technique. We
                 discuss alternate implementations that have better
                 numerical stability properties and can lead to better
                 predictions. Our results will be illustrated by looking
                 at an application involving prediction of galaxy
                 redshift from broadband spectrum data.",
  URL =         "http://www.jmlr.org/jmlr.xml;
                 http://www.jmlr.org/style.css;
                 http://www.jmlr.org/papers/volume10/foster09a/foster09a.pdf;
                 http://www.jmlr.org; http://www.jmlr.org/; /papers;
                 /author-info.html; /news.html; /scope.html;
                 /editorial-board.html; /announcements.html;
                 /proceedings; /mloss; /search-jmlr.html; /manudb;
                 /jmlr.xml",
}

@techreport{Titsias2009,
  author = "Michalis K.\ Titsias",
  title = "Variational Model Selection for Sparse Gaussian Process Regression",
  institution = "School of Computer Science, University of Manchester, UK",
  year = "2009",
  URL = "http://www.cs.manchester.ac.uk/~mtitsias/papers/sparseGPv2.pdf",
}
