\documentclass[10pt]{article}

% PACKAGES
\usepackage[usenames]{color}

\newcommand{\mail}{\mailto{markus.mottl@gmail.com}}
\newcommand{\athome}[2]{\ahref{http://www.ocaml.info/#1}{#2}}
\newcommand{\www}{\athome{}{Markus Mottl}}

% INCLUDE HEVEA SUPPORT
\usepackage{hevea}

%BEGIN LATEX
\usepackage{natbib}
%END LATEX

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{accents}

% HTML FOOTER
\htmlfoot{
  \rule{\linewidth}{1mm}
  Copyright \quad \copyright \quad 2009-
  \quad \www \quad \langle\mail\rangle
}

% HYPHENATION

\hyphenation{he-te-ro-ske-da-stic}
\hyphenation{ana-ly-ti-cally}
\hyphenation{know-ledge}

\DeclareMathAlphabet{\mathsfsl}{OT1}{cmss}{m}{sl}

\newcommand{\red}{\textcolor{red}}
\newcommand{\blue}{\textcolor{blue}}

\newcommand{\dif}{\mathrm{d}}

\newcommand{\myu}[1]{\underaccent{\bar}{#1}}

\newcommand{\onehalf}{\tfrac{1}{2}}

\newcommand{\mat}[1]{\mbox{$\mathsfsl{#1}$}}
\newcommand{\myvec}[1]{\mbox{\boldmath$#1$}}

\newcommand{\diagv}[1]{\mathrm{diag_v}(#1)}
\newcommand{\diagm}[1]{\mathrm{diag_m}(#1)}
\newcommand{\trace}[1]{\mathrm{tr}(#1)}
\newcommand{\transv}[1]{\myvec{#1}^\top}
\newcommand{\transm}[1]{\mat{#1}^\top}

\newcommand{\imat}[1]{\mat{#1^{-1}}}
\newcommand{\itransm}[1]{\mat{#1^{-\top}}}
\newcommand{\chol}[1]{\mat{#1^{\onehalf}}}
\newcommand{\cholt}[1]{\mat{#1^{\tfrac{\top}{2}}}}
\newcommand{\ichol}[1]{\mat{#1^{-\onehalf}}}
\newcommand{\icholt}[1]{\mat{#1^{-\tfrac{\top}{2}}}}

\newcommand{\Km}{\mat{K_M}}
\newcommand{\iKm}{\imat{K_M}}
\newcommand{\dKm}{\mat{\dot{K}_M}}
\newcommand{\dKn}{\mat{\dot{K}_N}}
\newcommand{\Knm}{\mat{K_{NM}}}
\newcommand{\Kmn}{\transm{K_{NM}}}
\newcommand{\uKnm}{\myu{\mat{K}}_{\mathsfsl{NM}}}
\newcommand{\uuKnm}{\myu{\myu{\mat{K}}}_{\mathsfsl{NM}}}
\newcommand{\dKnm}{\mat{\dot{K}_{NM}}}
\newcommand{\uKmn}{\myu{\mat{K}}_{\mathsfsl{NM}}^\top}

\newcommand{\dl}{\dot{l}}

\newcommand{\vecu}{\myvec{u}}
\newcommand{\vecr}{\myvec{r}}
\newcommand{\vecs}{\myvec{s}}
\newcommand{\vect}{\myvec{t}}
\newcommand{\vecw}{\myvec{w}}
\newcommand{\vecv}{\myvec{v}}
\newcommand{\vecvx}{\myvec{v}_1}
\newcommand{\vecvy}{\myvec{v}_2}
\newcommand{\vecy}{\myvec{y}}
\newcommand{\uvecy}{\myu{\vecy}}

\newcommand{\vecsdh}{\onehalf\myvec{\dot{s}}}
\newcommand{\vecis}{\myvec{s}^{-1}}
\newcommand{\veciss}{\myvec{s}^{-\onehalf}}

\newcommand{\matB}{\mat{B}}
\newcommand{\matI}{\mat{I}}
\newcommand{\matQ}{\mat{Q}}
\newcommand{\matQn}{\mat{\widetilde{Q}}}
\newcommand{\tmatQn}{\transm{\widetilde{Q}}}
\newcommand{\matR}{\mat{R}}
\newcommand{\matS}{\mat{S}}
\newcommand{\matT}{\mat{T}}
\newcommand{\matU}{\mat{U}}
\newcommand{\matUx}{\mat{U}_1}
\newcommand{\matUy}{\mat{U}_2}
\newcommand{\matV}{\mat{V}}
\newcommand{\matW}{\mat{W}}
\newcommand{\matWx}{\mat{W}_1}
\newcommand{\matX}{\mat{X}}
\newcommand{\matXx}{\mat{X}_1}
\newcommand{\matWy}{\mat{W}_2}
\newcommand{\matXy}{\mat{X}_2}

\newcommand{\Lam}{\mat{\Lambda}}
\newcommand{\Lamss}{\mat{\Lambda}_{\sigma^2}}
\newcommand{\Lamssi}{\imat{\Lambda_{\sigma^2}}}

% TITLE

\title{Gaussian Process Regression with OCaml\\Version 0.9}

\author{Markus Mottl\footnote{\mail}}

\date{\today}

% DOCUMENT
\begin{document}

\maketitle

\begin{abstract}

This manual documents the implementation and use of the OCaml GPR
library for Gaussian Process Regression with OCaml.

\end{abstract}

\section{Overview}

The OCaml GPR library features implementations of many of the latest
developments in the currently heavily researched machine learning
area of Gaussian process regression.

\subsection{Background}

Gaussian processes define probability distributions over functions
as prior knowledge.  Bayesian inference can then be used to compute
posterior distributions over these functions given data, e.g.\ to
solve regression problems\footnote{Gaussian processes can also be
used for classification purposes.  This is by itself a large research
area, which is not covered by this library.}.  As more data becomes
available, a Gaussian process framework learns an ever more accurate
distribution of functions that generate the data.\\

Due to their mathematically elegant nature, Gaussian processes allow
for analytically tractable calculation of the posterior mean and
covariance functions.  Though it is easy to formulate the required
equations, GPs come at a usually intractably high computational
price for large problems.  Good approximation methods have been
developed in the recent past to address this shortcoming, and this
library makes heavy use of them.\\

Gaussian processes are true generalizations of e.g.\ linear regression,
ARMA processes, single-layer neural networks with infinitely many
hidden units, etc., and thus capable of replacing numerous less
general approaches.  They are closely related to support vector-
(SVM) and other modern kernel machines, but have features that may
make them a more suitable choice in many situations, for example
predictive variances, Bayesian model selection, \ldots\\

It would go beyond the scope of this library documentation to provide
for a detailed treatment of Gaussian processes.  Hence, readers
unfamiliar with this approach may want to consult a wealth of online
resources.  This subsection presents an overview of recommended
materials.

\subsubsection{Video tutorials}

Video tutorials are probably best suited for quickly developing an
intuition and basic formal background of Gaussian processes and
perspectives for their practical use.

\begin{itemize}

\item
\emph{\footahref{http://videolectures.net/gpip06\_mackay\_gpb}{Gaussian
Process Basics}}: David MacKay's lecture given at the \emph{Gaussian
Processes in Practice Workshop} in 2006.  This one hour video
tutorial uses numerous graphical examples and animations to aid
understanding of the basic principles behind inference techniques
based on Gaussian processes.

\item
\emph{\footahref{http://videolectures.net/epsrcws08\_rasmussen\_lgp}{Learning
with Gaussian Processes}}: a slightly longer, two hour video tutorial
series presented by Carl Edward Rasmussen at the Sheffield EPSRC
Winter School 2008, which goes into slightly more detail.

\item
\emph{\footahref{http://videolectures.net/mlss07\_rasmussen\_bigp}{Bayesian
Inference and Gaussian Processes}}: readers interested in a fairly
thorough, from the ground up treatment of Bayesian inference
techniques using Gaussian processes may want to watch this five
hour video tutorial series presented by Carl Edward Rasmussen at
the MLSS 2007 in T\"ubingen.

\end{itemize}

\subsubsection{Books and papers}

The following texts are intended for people who need a more formal
treatment and theory.  This is especially recommended if you want
to be able to implement Gaussian processes and their approximations
efficiently.

\begin{itemize}

\item
\emph{\footahref{http://www.gatsby.ucl.ac.uk/\home{snelson}/thesis.pdf}{Flexible
and efficient Gaussian process models for machine learning}}: Edward
Lloyd Snelson's PhD thesis (\cite{SnelsonThesis}) offers a particularly
readable treatment of modern inference and approximation techniques
that avoids heavy formalism in favor of intuitive notation and
clearly presented high-level concepts without sacrificing detail
needed for implementation.  This library owes a lot to his work.

\item \emph{\footahref{http://www.gaussianprocess.org/gpml}{Gaussian
Processes for Machine Learning}}: many researchers in this area
would call this book written by Carl Edward Rasmussen and Christopher
K.\ I.\  Williams the ``bible of Gaussian processes''.  It presents
a rigorous treatment of the underlying theory for both regression
and classification problems, and more general aspects like properties
of covariance functions, etc.  The authors have kindly made the
full text and Matlab sources available online.  Their
\footahref{http://www.gaussianprocess.org}{Gaussian process website}
also lists a great wealth of other resources valuable for both
researchers and practitioners.

\end{itemize}

References to research about specific techniques used in the OCaml
GPR library are provided in the bibliography.

\subsection{Features of OCaml GPR}

Among other things the OCaml GPR library currently offers:

\begin{itemize}

\item Sparse Gaussian processes using the FI(T)C\footnote{\emph{Fully
Independent (Training) Conditional}} approximation for computationally
tractable learning (see \cite{conf/nips/2005}, \cite{SnelsonThesis}).
Unlike some other approximations that lead to degeneracy, this one
maintains sane posterior variances.

\item Optimization of hyper parameters by evidence
maximization\footnote{Also known as type II maximum likelihood.},
including optimization of inducing inputs (SPGP algorithm\footnote{This
library exploits sparse matrix operations to achieve optimum big-O
complexity when learning inducing inputs with the SPGP algorithm,
but also for multiscales and other hyper parameters that imply
sparse derivative matrices.}).

\item Supervised dimensionality reduction, and improved predictions
under heteroskedastic noise conditions (see \cite{conf/uai/SnelsonG06},
\cite{SnelsonThesis}).

\item Sparse multiscale Gaussian process regression (see
\cite{conf/icml/WalderKS08}).

\item Variational improvements to the approximate posterior
distribution (see \cite{Titsias2009}).

\item Numerically stable GP calculations using QR-factorization to
avoid the more commonly used and numerically unstable solution of
normal equations via Cholesky factorization (see \cite{Foster2009}).

\item Consistent use of BLAS/LAPACK throughout the library for
optimum performance.

\item Functors for plugging arbitrary covariance functions (=
kernels) into the framework.  There is no constraint on the type
of covariance functions, i.e.\ also string inputs, graph inputs,
etc., could potentially be used with ease given suitable covariance
functions\footnote{The library is currently only distributed with
covariance functions that operate on multivariate numerical inputs.
Feel free to contribute others.}.

\item Rigorous test suite for checking both user-provided derivatives
of covariance functions, which are usually quite hard to implement
correctly, and self-test code to verify derivatives of log-likelihood
functions using finite differences.

\end{itemize}

\section{API documentation}

\section{Example application}

\section{Internals}

\section{FIC computations}

This section consists of equations used for computing the FI(T)C
predictive distribution, and the log-likelihood and its derivatives
in the OCaml GPR library.  The implementation factorizes the
computations in this way for several reasons: to minimize computation
time and memory usage, and to improve numerical stability by using
QR factorization to avoid normal equations, and by avoiding inverses
whenever possible without great loss of efficiency.  It otherwise
aims for ease of implementation, e.g.\ combining derivative terms
to simplify dealing with sparse matrices.\\

The presentation and notation here is somewhat similar to
\cite{SnelsonThesis}.  Thus, interested readers are encouraged to
first read his work, especially the derivations in the appendix.
Our presentation deviates in minor ways, but should hopefully still
be fairly easy to compare.  The log-likelihood derivatives have
been heavily restructured though.  The mathematical derivation of
this restructuring would be extremely tedious, hence only the final
result is presented.\\

Here are a few definitions:

\begin{itemize}

\item $\mathrm{diag_m}$ is the function that returns the matrix
consisting of only the diagonal of a given matrix.  $\mathrm{diag_v}$
returns the diagonal as a vector.

\item $\otimes$ represents element-wise multiplication of vectors.
A vector raised to a power means element-wise application of that
power.

\item Parts in \red{red} represent terms used for Michalis K.\
Titsias' variational improvement (see \cite{Titsias2009}) to the
posterior marginal likelihood.

\item Parts in \blue{blue} provide for an alternative, more compact,
direct and hence more efficient way of computing some result if the
required parameters are already available.

\end{itemize}

\begin{eqnarray*}
\matV & = & \Knm\icholt{K_M} \\
\mat{\widetilde{K}_N} & = & \matV \transm{V} \\
\Lam & = & \diagm{\mat{K_N} - \mat{\widetilde{K}_N}} \\
\Lamss & = & \Lam + \sigma^2 \matI \\
\\
\vecr & = & \diagv{\Lam} \\
\vecs & = & \diagv{\Lamss} \\
\\
\uKnm & = & \ichol{\Lamss} \Knm \\
\matQ \matR & = & {\uKnm\choose\cholt{K_M}} \hspace{5mm}
\textrm{(QR-factorization of $\uKnm$ stacked on $\cholt{K_M}$)} \\
\\
\matB & = & \Km + \uKmn\uKnm = \transm{R}\transm{Q} \mat{Q} \matR = \transm{R}\matR \\
\matQn & = & {\lfloor \matQ \rfloor\footnotemark}_{N} \longrightarrow \uKnm = \matQn \matR \\
\matS & = & \ichol{\Lamss}\matQn\itransm{R} \\
\\
l_1 & = & -\onehalf (\log|\matB| - \log|\Km| + \log|\Lamss| + N \log 2\pi) \red{+ -\onehalf\vecis \cdot \vecr} \\
\\
\uvecy & = & \veciss \otimes \vecy \\
\vect & = & \imat{R} \tmatQn \uvecy \\
\vecu & = & \uvecy - \matQn \tmatQn \uvecy \\
\\
l_2 & = & \blue{-\onehalf \vecu\cdot\uvecy} = -\onehalf(\|\uvecy\|^2 - \|\tmatQn \uvecy\|^2) \\
l & = & l_1 + l_2 \\
\\
\matT & = & \imat{K_M} - \imat{B} \\
\\
\mu_* & = & \mat{K_{*M}} \vect \\
\sigma^2_* & = & K_* - \mat{K_{*M}}\matT\transm{K_{*M}} + \sigma^2 \matI \\
\end{eqnarray*}
\footnotetext{Take first $N$ rows.}

\begin{eqnarray*}
\matU & = & \mat{V} \ichol{K_M} \\
\\
\vecvx & = & \vecis \otimes (\vec{1} \red{\, + \, \vec{1} - \vecis \otimes \vecr} - \diagv{\matQn\tmatQn}) \\
\matUx & = & \diagm{\vecvx^{\onehalf}} \matU \\
\matWx & = & \matT - \matUx^\top\matUx \\
\matXx & = & \matS - \diagm{\vecvx}\matU \\
\dl_1 & = & -\onehalf(\vecvx \cdot \diagv{\dKn} - \trace{\transm{W}_1\dKm}) - \trace{\transm{X}_1\dKnm} \\
\\
\vecw & = & \veciss \otimes \vecu \\
\vecvy & = & \vecw \otimes \vecw \\
\matUy & = & \diagm{\vecw} \matU \\
\matWy & = & \vect \vect^\top - \matUy^\top\matUy \\
\matXy & = & \vecw\vect^\top - \diagm{\vecvy}\matU \\
\dl_2 & = & \onehalf(\vecvy \cdot \diagv{\dKn} - \trace{\transm{W}_2\dKm}) + \trace{\transm{X}_2\dKnm} \\
\\
\dl & = & \dl_1 + \dl_2 \\
\\
\tfrac{\partial l_1}{\partial\sigma^2} & = & -\onehalf(\mathrm{sum}(\vecvx) \red{\, - \, \mathrm{sum}(\vecis)}) \\
\tfrac{\partial l_2}{\partial\sigma^2} & = & \onehalf\mathrm{sum}(\vecvy) \\
\tfrac{\partial l}{\partial\sigma^2} & = & \tfrac{\partial l_1}{\partial\sigma^2} + \tfrac{\partial l_2}{\partial\sigma^2}
\end{eqnarray*}
\blue{
\begin{eqnarray*}
\vecv & = & \vecvx - \vecvy  \\
\matW & = & \matWx - \matWy = \matT - \vect \transv{t} - \matUx^\top\matUx + \matUy^\top\matUy \\
\matX & = & \matXx - \matXy = \matS - \vecw \transv{t} - \diagm{\vecv}\matU \\
\dl & = & -\onehalf(\vecv \cdot \diagv{\dKn} - \trace{\transm{W}\dKm}) - \trace{\transm{X}\dKnm} \\
\\
\tfrac{\partial l}{\partial\sigma^2} & = & -\onehalf(\mathrm{sum}(\vecv) \red{\, - \, \mathrm{sum}(\vecis)}) \\
\end{eqnarray*}
}

\newpage

\section{Notes/reminders for future work}

Log-derivatives:

\begin{itemize}
\item $\tfrac{\partial f}{\partial \log(x)} = \tfrac{\partial f}{\partial x} x$
\end{itemize}

\subsection{Nonlinear clustering:}

\begin{itemize}
\item $k(x, y) = \langle \phi(x) | \phi(y) \rangle$
\item $\|\phi(x) - \phi(y)\|^2 = k(x,x)-2k(x,y)+k(y,y)$
\item find one inducing point
\item choose point x farthest away wrt.\ k
\item choose antipodal point y to x wrt.\ k
\item determine for all points to which of x or y they are closer
\item create two clusters
\item recurse
\item when suitable granularity reached, use PI(T)C
\end{itemize}

% BIBLIOGRAPHY
\bibliographystyle{alpha}
\bibliography{gpr}
 
\end{document}
